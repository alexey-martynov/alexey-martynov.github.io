---
layout: post
title: Multithreaded Queue in C++
permalink: /posts/c++-mt-queue.html
---

Today concurrent programming is an important technique. Multicore CPUs
are available everywhere. But platform support for concurrent
programming idioms and patterns is very different. Passing data
between threads often requires a queue which can be used from
different threads. How many pitfalls may exist in such queue in C++?

> NOTE: this article is under rework to use newer standard C++
> than 2003.

# Preface 

As example of concurrency the I/O handling on different platforms
might be taken. I/O multiplexing on Microsoft Windows is performed via
asynchronous operations and I/O Completion Ports, but on UNIX-like
systems non-blocking I/O is implemented using `select()`, `poll()`,
`epoll` (Linux) or `kqueue` (BSD) facilities. The two obvious
differences are:

* When the system raises an event the Windows I/O operation is done
  and a buffer is populated (read) or written (write). But POSIX
  requires to perform an I/O operation manually after receiving an
  event from the system.

* The Windows I/O Completion Port can distribute I/O events from the
  single port to a thread pool. POSIX facilities don't have such
  ability: if you call, for example, `select()` from more than one
  thread with the same set of descriptors the system wakes up all
  threads with the same set of events simultaneously.

These differences require different implementation of I/O
demultiplexing but it can be encapsulated in generic library. Usually,
the problem with simultaneous calls to `select()` can be solved by
using single I/O thread which performs I/O and I/O event
demultiplexing and a thread pool for handling data and requests. To
pass information between I/O thread and processing threads in the pool
a multithreaded queue is required. It has "single writer --- multiple
readers" type to pass data from the I/O thread to the processing
threads. To pass data in opposite direction it should have type
"multiple writers --- single reader".

A possible implementation of such multithreaded queue in C++ is
described in this article. This article won't describe implementation
of lock-free queue. This kind of containers are very hard to verify
and should be matter of a separate discussion.

> The article doesn't describe additional aspects that arise when 
> C++ 11 or newer standard is used like move semantics and
> `noexcept`.

# Acknowledgements

Special thanks to Yakov Belikov and Yuri Mukhitov for their help in
correcting mistakes.

# Objectives

1. Required: implement queue with concurrent access and FIFO
   semantics.
   
2. Required: "multiple writers --- multiple readers" semantics.

3. Optional: allow deduplication semantics in queue.

4. Optional: implement prioritization.

# Primitive Implementations

> For clarity, in this article class `QueueItem` used as queue
> element.

The most straight-forward and naive way to create a multithreaded
queue may look like:

```c++
#include <deque>
#include <mutex>

std::deque<QueueItem> queue;
std::mutex queue_mutex;

  // Somewhere in the code
  // Put data to the queue
  {
    std::unique_lock lock(queue_mutex);

    queue.push_back(QueueItem(...));
  }

  // Get data from the queue
  QueueItem item;
  bool has_item = false;

  {
    std::unique_lock lock(queue_mutex);

    if (!queue.empty()) {
      item = queue.front();
      queue.pop_front();
      has_item = true;
    }
  }
  if (has_item) {
    // Handle item
    ...
  }
```

This example lacks encapsulation. An every queue client shouldn't
forget proper locking and checking for queue emptiness. Also if the
client wants to wait for an element availability waiting semantics
should be added.

In any case this example shows important technique for C++
programmers: how to control lifetime of object when using "RAII"
(Resource Acquisition Is Initialization) idiom. The scope of our locks
should be as small as possible so we place them inside an extra block.

The next stage is encapsulation of the queue logic to single
class. Assuming that clients should wait while queue is empty, it may
look like:

```c++
#include <condition_variable>
#include <deque>
#include <mutex>

template <typename T>
class mtqueue_1
{
public:
  // We need this constructor because we should prevent copying
  // of this queue objects. To implement this requirement we
  // declaring private copying constructor and assignment operator.
  // Unfortunately, this effectively eliminates auto-generated
  // default constructor.
  mtqueue_1()
  {}
  // Prevent object copying.
  mtqueue_1(const mtqueue_1< T > &) = delete;
  mtqueue_1< T > & operator =(const mtqueue_1< T > &) = delete;

  void enqueue(const T & value)
  {
    std::unique_lock lock(queue_mutex)_;

    queue_.push_back(value);
    if (queue.size() == 1u) {
      queue_cond_.notify_all();
    }
  }

  T get()
  {
    std::unique_lock lock(queue_mutex_);

    while (queue_.empty()) {
      queue_cond_.wait(lock);
    }

    T result(queue_.front());

    queue_.pop_front();
    return result;
  }

private:
  std::deque< T > queue_;
  std::mutex queue_mutex_;
  std::condition_variable queue_cond_;
};
```

This implementation is much better because:

* All queue logic is encapsulated inside. Nobody can forget to unlock
  queue mutex.
  
* Clients properly wait when queue is empty. Nobody can break logic of
  waiting. It is a very common mistake to forget that POSIX
  synchronization primitives like `condition_variable` can awake from
  waiting without signaling.

In function `enqueue()` we lock queue mutex push an element and if and
only if the queue has size equal to 1 we try to awake clients. We
can't awake one client because if all clients are ready to process but
nobody will be awaken before next element is put to queue then only
one element will be processed. If the queue has size more than 1 it
means that nobody is ready to process the queue and we can skip
signalling via condition. We don't need to broadcast a signal because
we put to the queue only one element and only one thread can handle
it.

In function `get()` we are performing the following steps:

1. Lock mutex.

2. Check size of the queue.

3. If the queue is empty we are waiting on the condition.

4. After awake we check again that queue is not empty.

5. If the queue is still empty it means that we have spurious wake up
   and can go to sleeping again.

6. If the queue is not empty, we get the front element, remove it from
   queue and return it.

Although we have improved our queue a lot we have introduced some new
issues to our queue implementation:

* It is not possible now to signal to client about shutting down of
  processing. This logic should be encapsulated to the `QueueItem` and
  should be placed to queue multiple times (one time per reading
  client).

* We lack strong exception safety. If copying constructor of
  `QueueItem` throws exception when we returning object we lose
  object.

All these issues should be resolved together and we will try to do it
in next chapters.

# Increasing Exception Safety

To improve exception safety we should guarantee that no throwing
operations can be performed after an object is removed from queue and
before it passed to client.

This can be achieved, for example, by returning a pointer. The naive
way to return a reference to an object is bad idea because it is
impossible to find correct object placement: object as local variable
will die after return from function and we will have an invalid
reference, reference to the queue container breaks encapsulation of
locking.

We shouldn't use raw pointers here because we should clearly describe
ownership of this object. `std::unique_ptr` can solve this problem for
us.

Please look at the following example (changes in logic are marked as diff):</p>

```diff
  #include <condition_variable>
  #include <deque>
+ #include <memory>
  #include <mutex>
  
  template < typename T >
  class mtqueue_2
  {
  public:
    // We need this constructor because we should prevent copying
    // of this queue objects. To implement this requirement we
    // declaring private copying constructor and assignment operator.
    // Unfortunately, this effectively eliminates auto-generated
    // default constructor.
    mtqueue_2()
    {}
    // Prevent object copying.
    mtqueue_2(const mtqueue_2< T > &) = delete;
    mtqueue_2< T > & operator =(const mtqueue_2< T > &) = delete;
    
    void enqueue(const T & value)
    {
      std::unique_lock lock(queue_mutex_);
    
      queue_.push_back(value);
      if (queue_.size() == 1u) {
        queue_cond_.notify_all();
      }
    }
    
-   T get()
+   std::unique_ptr< T > get()
    {
      std::uniqe_lock lock(queue_mutex_);
    
      while (queue_.empty()) {
        queue_cond_.wait(lock);
      }
    
-     T result(queue_.front());
+     std::unique_ptr< T > result(new T(queue_.front()));
    
      queue_.pop_front();
      return result;
    }
  
  private:
    std::deque< T > queue_;
    std::mutex queue_mutex_;
    std::condition_variable queue_cond_;
  };
```

The case which isn't covered is proper canceling awaiting. For
example:

```diff
  #include <condition_variable>
  #include <deque>
  #include <memory>
  #include <mutex>
  
  template < typename T >
  class mtqueue_3
  {
  public:
    mtqueue_3():
+     shutdown_(false)
    {}
    // Prevent object copying.
    mtqueue_3(const mtqueue_3< T > &) = delete;
    mtqueue_3< T > & operator =(const mtqueue_3< T > &) = delete;
  
    void enqueue(const T & value)
    {
      std::unique_lock lock(queue_mutex_);
  
+     if (!shutdown_) {
        queue_.push_back(value);
          if (queue_.size() == 1u) {
            queue_cond_.notify_all();
          }
+     }
    }
  
    std::unique_ptr< T > get()
    {
      std::unique_lock lock(queue_mutex_);
  
-     while (queue_.empty()) {
+     while (!shutdown_ && queue_.empty()) {
        queue_cond_.wait(lock);
      }
  
+     if (shutdown_) {
+       return std::unique_ptr< T >();
+     }
  
      std::unique_ptr< T > result(new T(queue_.front()));
  
      queue_.pop_front();
  
      return result;
    }
  
+   void shutdown_processing()
+   {
+     std::unique_lock lock(queue_mutex_);
+ 
+     shutdown_ = true;
+     queue_cond_.notify_all();
+   }
  
  private:
    std::deque< T > queue_;
    std::mutex queue_mutex_;
    std::condition_variable queue_cond_;
+   bool shutdown_;
  };
```

This solution returns empty pointer when queue should be
stopped. Usually it isn't good practice to give additional semantics
to return value but fixing this issue can lead us to other design
issues. Look at the following implementations
of `get()` function:

```diff
- T get()
+ bool get(T & result)
  {
    std::unique_lock lock(queue_mutex_);
  
    while (!shutdown_ && queue_.empty()) {
      queue_cond_.wait(lock);
    }
  
+   if (shutdown_) {
+     return false;
+  	}
  
-   T result(queue_.front());
+   result = queue_.front();
  
    queue_.pop_front();
  
-   return result;
+   return true;
  }
  
- T get()
+ bool get(std::unique_ptr< T > & result)
  {
    std::unique_lock lock(queue_mutex_);
  
    while (!shutdown_ && queue_.empty()) {
      queue_cond_.wait(lock);
    }
  
+   if (shutdown_) {
+     result.reset();
+     return false;
+   }
  
- 	T result(queue_.front());
+   result_.reset(new T(queue.front()));
  
  	queue_.pop_front();
  
-   return result;
+   return true;
  }
```

Both functions use their argument as output argument and this usually
looks ugly in client code. First one also requires a properly
constructed instance to call it.

As intermediate resume: we have implementation which offers strong
exception safety and safe concurrent access from multiple threads. But
client code looks strange and the following limitations are placed:

* Client code should check for special case&nbsp;--- shutdown mode.

* An extra memory allocation required to implement strong exception
  safety.

* Copying constructor for queue element is called twice.

Also no optional requirements are satisfied.

# Side Step

This chapter requires a small mental trick. To do it we should answer
following question: do we really need to return an item from the queue to process it?

The answer is "No". We can extract handling code to a separate
function and call it when an item is ready to be processed.

Take a look at the following implementation:

```diff
  #include <condition_variable>
  #include <deque>
  #include <memory>
  #include <mutex>
  
  template < typename T >
  class mtqueue_4
  {
  public:
    mtqueue_4():
      shutdown_(false)
    {}
    // Prevent object copying.
    mtqueue_4(const mtqueue_4< T > &) = delete;
    mtqueue_4< T > & operator =(const mtqueue_4< T > &) = delete;
  
    void enqueue(const T & value)
    {
      std::unique_lock lock(queue_mutex_);
  
      if (!shutdown_) {
        queue_.push_back(value);
        if (queue_.size() == 1u) {
          queue_cond_.notify_all();
        }
      }
    }
  
-   std::unique_ptr< T > get()
-   {
-     std::unique_lock lock(queue_mutex_);
- 
-     while (!shutdown_ && queue_.empty()) {
-       queue_cond_.wait(lock);
-     }
- 
-     if (shutdown_) {
-       return std::unique_ptr< T >();
-     }
- 
-     std::unique_ptr< T > result(new T(queue_.front()));
- 
-     queue_.pop_front();
- 
-     return result;
-   }
+   template < typename Callback >
+   void process(Callback&& callback, bool wait = true)
+   {
+     bool process_queue = true;
+ 
+     while (process_queue) {
+       std::unique_ptr< T > item;
+ 
+       {
+         std::unique_lock lock(queue_mutex_);
+ 
+         while (!shutdown_ && queue_.empty() && wait) {
+           queue_cond_.wait(lock);
+         }
+ 
+         if (shutdown_) {
+           process_queue = false;
+         } else if (!queue_.empty()) {
+           item.reset(new T(queue_.front()));
+           queue_.pop_front();
+         }
+       }
+ 
+       if (item) {
+         process_queue = callback(*item);
+       }
+     }
+   }
  
    void shutdown_processing()
    {
      std::unique_lock lock(queue_mutex_);
  
      shutdown_ = true;
      queue_cond_.notify_all();
    }
  
  private:
    std::deque< T > queue_;
    std::mutex queue_mutex_;
    std::condition_variable queue_cond_;
    bool shutdown_;
  };
```

The main change contains removing `get()` function and
introducing `process()` function. The new function
encapsulates complete logic for handling queue. It has loop for
processing items. Inside the loop function tries to get an item
from the queue. If queue is empty and `wait`
is `false` function doesn't try to wait and
finishes. If the queue is empty and `wait`
is `true` the function waits for an item in queue or a
shutdown event. After obtaining an item from the queue callback
is called to process the item. If callback returns false function
stops the loop and returns. In the other case it tries to process
next element.

Current implementation doesn't remove extra allocation in
heap but make a small step toward functional style of queue. Which
solution `mtqueue_3` or `mtqueue_4`
better it is a matter of huge discussion. In the program with
large amount of components written in STL or functional style the
latter is better because it matches overall style of program.

Please note that following chapters show various changes to queue
design which can be debatable. They are used in this article to
show how far can you go to satisfy requirements and what C++ can
offer you on this way.

# Satisfying Optional Requirements

Our optional requirements are:

* Allow deduplication semantics in queue.

* Implement prioritization.

The first requirement is very simple. We should allow somebody to
check whether new item should be put to the queue or not. It would
be better to use extra policy template parameter to handle this
logic because "duplicate" criteria might be less comprehensive than
equality.

For example:

```diff
  #include <condition_variable>
  #include <deque>
  #include <memory>
  #include <mutex.hpp>
  
  template < typename T, typename Policy >
  class mtqueue_5
  {
  public:
    mtqueue_5():
      shutdown(false)
    {}
  
+   mtqueue_5(const Policy & policy):
+     policy_(policy),
+     shutdown_(false)
+   {}
  
    void enqueue(const T & value)
    {
      std::unique_lock lock(queue_mutex_);
  
-     if (!shutdown_) {
+     if (!shutdown_ && policy_.should_enqueue(value, queue_.begin(), queue_.end())) {
        queue_.push_back(value);
        if (queue_.size() == 1u) {
          queue_cond_.notify_all();
        }
      }
    }
  
    ...
  private:
    ...
+   Policy policy_;
    ...
  };
```

The new implementation uses policy class which should contain function
`should_enqueue()`. This function accepts new value to put to the
queue and contents of queue in terms of iterator range. The function
should return `true` when new item should be placed to queue and
`false` otherwise. This function *can* modify elements in the queue,
for example, to deduplicate events and update event inside queue with
new data.

The new constructor allows to use policy with non-default
constructor.

The second optional requirement is slightly harder to implement. First
thing to decide is: is prioritization means storing objects according
priorities or it means processing objects out of order?

If we need to support real priorities and store objects in order
of priorities we should extend `enqueue()` and policy
class. For example:

```diff
+ #include <algorithm>
+ #include <functional>
  	...
    void enqueue(const T &amp; value)
    {
      std::unique_lock lock(queue_mutex_);
  
      if (!shutdown_ && policy_.should_enqueue(value, queue_.begin(), queue_.end())) {
-       queue_.push_back(value);
+       if (Policy::is_priority_queue()) {
+         std::deque< T >::iterator place = std::find_if(queue_.begin(), queue_.end(),
+             priority_less(value));
+ 
+         queue_.insert(place, value);
+       } else {
+         queue_.push_back(value);
+       }
  			
        if (queue_.size() == 1u) {
          queue_cond-.notify_all();
        }
      }
    }
    ...
  private:
+   class priority_less:
+     public std::unary_function< T, bool >
+   {
+   public:
+     priority_less(const T & value):
+       value(value)
+     {}
+ 
+     bool operator ()(const T & item) const
+     {
+       return Policy::compare_priorities(value, item) < 0;
+     }
+ 
+   private:
+     const T & value;
+   };
    ...
```

The policy class is extended by adding 2 static functions:
`is_priority_queue()` and `compare_priorities()`. First one should
return `true` if queue should handle priorities. The second function
takes two objects and compares their priorities.

This solution is good when exact list of priorities is not exists. But
when list of priorities is fixed and relatively small making separate
queues for each priority is more effective way.

But this doesn't allow out of order processing. For example, we can't
handle same types of events simultaneously but we can handle different
types of events. This is not priority but dynamic lock for some type
of events. So changes should be done inside `process()` function. For
example:

```c++
  ...
  template < typename Fetcher, typename Callback >
  void process(Fetcher&& fetch, Callback&& callback, bool wait = true)
  {
    bool process_queue = true;

    while (process_queue) {
      std::unique_ptr< T > item;

      {
        std::unique_lock lock(queue_mutex_);

        if (shutdown_) {
          return;
        }

        // Fetch objects according to policy as shared state via fetch function
        std::deque< T >::iterator place = fetch(queue_.begin(), queue_.end());

        if ((place == queue_.end()) && wait) {
          queue_cond_.wait(lock);
          continue;
        }

        item.reset(new T(*place));
        queue_.erase(place);
      }

      process_queue = callback(*item);
    }
  }
  ...
```

The `fetch()` functor is called under queue lock to select an item to
get from queue. If it returns an ending iterator no items can be got
from queue. In this case we are trying to sleep until something
changes&nbsp;--- a new item is inserted to queue or another item
processing is finished. After awake the next try is performed.

But this code is not so good because it relies on correct state update
inside `fetch()` and inside `callback()`. This practice is not very
good. To improve overall style we should refactor code to ensure that
proper calls are made. We have 2 states here&nbsp;--- global for queue
and local for processor. Nothing can be done for local state without
imposing too much requirements on processing functor. Everybody who
needs local state should bind it manually. But the global state can be
stored inside the policy of queue. In this case we can add two extra
calls on beginning and on ending of item processing. This calls should
be done under queue lock to ensure proper state update. The `fetch()`
functor can receive policy object as first parameter to make decision
based on this state.

```diff
    ...
    template < typename Fetcher, typename Callback >
    void process(Fetcher&& fetch, Callback&& callback, bool wait = true)
    {
      bool process_queue = true;
+     std::unique_ptr< T > item;
  
      while (process_queue) {
-       std::unique_ptr< T > item;
  
        {
          std::unique_lock lock(queue_mutex_);
  
+         if (item) {
+           if (policy_.processing_ended(*item)) {
+             // More than one thread can handle objects from queue.
+             // So try to awake them.
+             queue_cond_.notify_all();
+           }
+           item.reset();
+         }
  
          if (shutdown_) {
            return;
          }
  
          // Fetch objects according to policy as shared state via fetch function
-         std::deque< T >::iterator place = fetch(queue_.begin(), queue_.end());
+         std::deque< T >::iterator place = fetch(policy_, queue_.begin(), queue_.end());
  
          if ((place == queue_.end()) && wait) {
            queue_cond_.wait(lock);
            continue;
          }
  
          item.reset(new T(*place));
          queue_.erase(place);
          <ins>policy_.processing_started(*item);</ins>
        }
  
-       <del>process_queue = callback(*item);</del>
+       if (!(process_queue = callback(*item))) {
+         std::unique_lock lock(queue_mutex_);
+ 
+         policy.processing_ended(*item);
+       }
      }
    }
    ...
```

If `Policy::processing_ended()` returns `true` it means that ending of
processing of the current item unlocks more than one class of events
which can be processed concurrently. So we should try to awake other
threads.

Full edition:

```c++
#include <algorithm>
#include <condition_variable>
#include <deque>
#include <functional>
#include <memory>
#include <mutex>

template < typename T, typename Policy >
class mtqueue_5
{
public:
  mtqueue_5():
    shutdown_(false)
  {}

  mtqueue_5(const Policy & policy):
    policy_(policy),
    shutdown_(false)
  {}

  // Prevent object copying.
  mtqueue_5(const mtqueue_5< T, Policy > &) = delete;
  mtqueue_5< T, Policy > & operator =(const mtqueue_5< T, Policy > &) = delete;
    
  void enqueue(const T & value)
  {
    std::unique_lock lock(queue_mutex_);

    if (!shutdown_ && policy_.should_enqueue(value, queue_.begin(), queue_.end())) {
      if (Policy::is_priority_queue()) {
        std::deque< T >::iterator place = std::find_if(queue_.begin(), queue_.end(),
            priority_less(value));

        queue_.insert(place, value);
      } else {
        queue_.push_back(value);
      }

      // We can't optimize out this call because fetcher
      // can prevent spare threads to handle data.
      queue_cond_.notify_all();
    }
  }

  template < typename Fetcher, typename Callback >
  void process(Fetcher&& fetch, Callback&& callback, bool wait = true)
  {
    bool process_queue = true;
    std::unique_ptr< T > item;

    while (process_queue) {
      {
        std::unique_lock lock(queue_mutex_);
      
        if (item.get()) {
          if (policy_.processing_ended(*item)) {
            // More than one thread can handle objects from queue.
            // So try to awake them.
            queue_cond_.notify_all();
          }
          item.reset();
        }
      
        if (shutdown_) {
          return;
        }
      
        // Fetch objects according to policy as shared state via fetch function
        std::deque< T >::iterator place = fetch(policy_, queue_.begin(), queue_.end());
      
        if ((place == queue_.end()) && wait) {
          queue_cond_.wait(lock);
          continue;
        }
      
        item.reset(new T(*place));
        policy_.processing_started(*item);
        queue_.erase(place);
      }

      if (!(process_queue = callback(*item))) {
        std::unique_lock lock(queue_mutex_);

        policy_.processing_ended(*item);
      }
    }
  }

  void shutdown_processing()
  {
    std::unique_lock lock(queue_mutex_);

    shutdown_ = true;
    queue_cond_.notify_all();
  }

private:
  class priority_less:
    public std::unary_function< T, bool >
  {
  public:
    priority_less(const T & value):
      value(value)
    {}

    bool operator ()(const T & item) const
    {
      return Policy::compare_priorities(value, item) < 0;
    }

  private:
    const T & value;
  };

  Policy policy_;
  std::deque< T > queue_;
  std::mutex queue_mutex_;
  std::condition_variable queue_cond_;
  bool shutdown_;
};
```

As you can see solution becomes more complicated.

# Memory Allocations

What is the main reason for intermediate object copying to the heap?

The main reason is too high abstraction with `std::deque`. All
standard containers don't support "detaching<" objects from them and
this requires copying (or moving). We can avoid such problem by
writing container manually.

Anybody should avoid writing containers if any standard container is
suitable. This is the main reason all previous solutions use
`std::deque`. But now we are trying to go further and `std::deque`
doesn't satisfy our new requirement&nbsp;--- detaching element. It is
possible to resolve this problem by using `std::list` and `splice()`
call but I leave it as exercise for everybody who interested in such
solution.

The simplest container that we can create is single linked
list. Usually it is implemented by introducing a structure with data
and pointer to the next element in list. This list can be easily
traversed from head to tail but traversing in reverse direction is
very costly. We simulate first object in queue with list head and last
object in queue with tail. To improve insertion efficiency we will
store pointer to list tail in another member.

The result will look like:

```diff
  #include <algorithm>
+ #include <cassert>
  #include <condition_variable>
  #include <functional>
- #include <deque>
+ #include<iterator>
  #include <mutex>
  
  template < typename T, typename Policy >
  class mtqueue_6
  {
+ private:
+   struct node_t
+   {
+     T value;
+     node_t * next;
+ 
+     node_t(const T & value):
+       value(value),
+       next(nullptr)
+     {}
+ 
+     // Prevent copying
+     node_t(const node_t &) = delete;
+     node_t & operator =(const node_t &);
+   };
  
  public:
+   class iterator:
+     public std::iterator< std::input_iterator_tag, T >
+   {
+   public:
+     bool operator ==(const iterator & rhs) const
+     {
+       return current_ == rhs.current_;
+     }
+ 
+     bool operator !=(const iterator & rhs) const
+     {
+       return current_ != rhs.current_;
+     }
+ 
+     iterator & operator ++()
+     {
+       assert(current_ != nullptr);
+ 
+       prev_ = current_;
+       current_ = current->next_;
+ 
+       return *this;
+     }
+ 
+     iterator operator ++(int)
+     {
+       assert(current != nullptr);
+ 
+       iterator result(*this);
+ 
+       ++(*this);
+ 
+       return result;
+     }
+ 
+     T & operator *() const
+     {
+       assert(current_ != nullptr);
+ 
+       return current->value;
+     }
+ 
+     T * operator ->() const
+     {
+       assert(current_ != nullptr);
+ 
+       return &current->value;
+     }
+ 
+   private:
+     node_t * prev_;
+     node_t * current_;
+ 
+     iterator(node_t * node):
+       prev_(nullptr),
+       current_(node)
+     {}
+ 
+     friend class mtqueue_6< T, Policy >;
+   };
  
    mtqueue_6():
+     head_(nullptr),
+     tail_(nullptr),
      shutdown_(false)
    {}
  
    mtqueue_6(const Policy & policy):
+     head_(nullptr),
+     tail_(nullptr),
      policy_(policy),
      shutdown_(false),
    {}
  
    // Prevent object copying.
    mtqueue_6(const mtqueue_6< T, Policy > &) = delete;
    mtqueue_6< T, Policy > & operator =(const mtqueue_6< T, Policy > &) = delete;
  
    void enqueue(const T & value)
    {
      std::unique_lock lock(queue_mutex_);
  
-     if (!shutdown_ && policy_.should_enqueue(value, queue_.begin(), queue_.end())) {
+     if (!shutdown_ && policy_.should_enqueue(value, iterator(head_), iterator(nullptr)>)) {
+       std::unique_ptr< node_t > node(new node_t(value));
  
        if (Policy::is_priority_queue()) {
-         std::deque< T >::iterator place = std::find_if(queue_.begin(), queue_.end(),
-                                                        priority_less(value));
+         iterator place = std::find_if(iterator(head_), iterator(nullptr), priority_less(node->value));
  
-         queue_.insert(place, value);
+         if (place.prev == nullptr) {
+           // Insert to head
+           node->next = head_;
+           head_ = node.get();
+           if (tail_ == nullptr) {
+             tail_ = node.get();
+           }
+         } else {
+           node->next = place.prev->next;
+           place.prev->next = node.get();
+           if (node->next == nullptr) {
+             tail_ = node.get();
+           }
+         }
+         node.release();
        } else {
-         queue_.push_back(value);
+         if (tail_ != nullptr) {
+           tail_->next = node.release();
+           tail_ = tail->next;
+         } else {
+           tail_ = head_ = node.release();
+         }
        }
  
        // We can't optimize out this call because fetcher
        // can prevent spare threads to handle data.
        queue_cond_.notify_all();
      }
    }
  
    template < typename Fetcher, typename Callback >
    void process(Fetcher&& fetch, Callback&& callback, bool wait = true)
    {
      bool process_queue = true;
-     std::unique_ptr< T > item;<
+     std::unique_ptr< node_t > item;
  
      while (process_queue) {
        {
          std::unique_lock lock(queue_mutex_);
  
          if (item.get()) {
-           if (policy_.processing_ended(*item)) {
+           if (policy_.processing_ended(item->value)) {
              // More than one thread can handle objects from queue.
              // So try to awake them.
              queue_cond_.notify_all();
            }
            item.reset();
          }
  
          if (shutdown_) {
            return;
          }
  
          // Fetch objects according to policy as shared state via fetch function
-         std::deque< T >::iterator place = fetch(policy_, queue_.begin(), queue_.end());
+         iterator place = fetch(policy_, iterator(head_), iterator(nullptr));
  
          if ((place == iterator(nullptr)) && wait) {
            queue_cond_.wait(lock);
            continue;
          }
+         assert(place.current_ != nullptr);
  
-         item.reset(new T(*place));
-         policy_.processing_started(*item);
+         policy_.processing_started(item->value);
  
-         queue_.erase(place);
+         // "element" and "prev" are owned by storage ("head_") until
+         // "head_" reassigned.
+         // Following operations are non-throwing so we can't lost element here.
+         if (place.prev_ == nullptr) {
+           assert(place.current_ == head_);
+ 
+           head_ = place.current->next;
+           if (tail_ == place.current_) {
+             tail_ = nullptr;
+           }
+         } else {
+           place.prev_->next = place.current_->next;
+           if (tail_ == place.current_) {
+             tail_ = place.prev_;
+           }
+         }
+         item.reset(place.current_);
        }
  
-       if (!(process_queue = callback(*item))) {
+       if (!(process_queue = callback(item->value))) {
          std::unique_lock lock(queue_mutex_);
  
-         policy_.processing_ended(*item);
+         policy_.processing_ended(item->value);
        }
      }
    }
  
    void shutdown_processing()
    {
      std::unique_lock lock(queue_mutex_);
  
      shutdown_ = true;
      queue_cond_.notify_all();
    }
  
  private:
    class priority_less:
      public std::unary_function< T, bool >
    {
    public:
      priority_less(const T & value):
        value(value)
      {}
  
      bool operator ()(const T & item) const
      {
        return Policy::compare_priorities(value, item) < 0;
      }
  
    private:
      const T & value;
    };
  
+   node_t * head_;
+   node_t * tail_;
    Policy policy_;
-   std::deque< T > queue;
    std::mutex queue_mutex_;
    std::condition_variable queue_cond_;
    bool shutdown_;
  };
```

We introduced here an iterator to traverse contents. The iterator has
a small trick&nbsp;--- iterator remembers previous element because
some operations like removing element from queue require it. All other
new logic is written to handle single list.

Now we have only one throwing operation before handling
element&nbsp;--- `processing_started()`. And we completely have
removed copying inside queue processing.

# Further Improvements

What problems we have in our queue design? Some of them can arise or
not depending on situation:

1. We can't reuse queue after `shutdown()` call.

2. We need copying constructor for object. It is possible to remove it
   in favor of moving depending on C++ standard version used.

3. We can't control how to queue allocates memory.

4. We require priority comparison function from policy even in case
  when queue is without priorities.

All this issues are resolved in last edition. But some techniques
should be discussed before.

Copying constructor can be avoided if we can use new feature from C++
2011&nbsp;--- r-value references and move constructors. If we not
lucky enough to use such nice feature we can fall back to a group of
template functions with different argument count and pass these
arguments to constructor of `node_t::value` inside `enqueue()`. The
r-value references already used in some places before and now it's
time to use them on queue data.

Memory allocation problem can be solved by passing an allocator to our
queue. But important change will arise&nbsp;--- allocator like
`std::allocator` makes object's initialization two-phase. As the
result `std::unique_ptr` becomes useless and we should create
"smarter" smart pointer. It has name `node_ptr`.

Priority comparison function can be avoided if we create 2 versions of
`enqueue()` function&nbsp;--- one to handle priorities and another to
place objects to the end of the queue. But these functions will have
same signature and won't compile. To resolve this issue we can use
SFINAE idiom (Substitution Failure Is Not An Error) via
`std::enable_if`. In the following sample this idiom is written
manually to show this technique. By extracting actual `enqueue()`
implementation to internal function we can hide this complexity from
our clients.

The same technique is used in calls to other callbacks in policy class.

```c++
#include <algorithm>
#include <condition_variable>
#include <functional>
#include <memory>
#include <mutex>
#include <utility>

namespace detail
{
	template <typename T>
	struct default_queue_policy
	{
		// Deduplication handler should have prototype like this:
		// template <typename InputIterator>
		// bool should_enqueue(const T &, const InputIterator &, const InputIterator &);

		// Element processing started event handler should have prototype like this:
		// void processing_started(const T &);

		// Element processing ended event handler should have prototype like this:
		// bool processing_ended(const T &);

		// Priorities comparator should have prototype like this:
		// int compare_priorities(const T &, const T &);

		// All these functions can be const-functions, non-const-functions or static
	};

	template <bool Value, typename T = void>
	struct select_type_t;

	template <typename T>
	struct select_type_t<true, T>
	{
		typedef T true_type;
	};

	template <typename T>
	struct select_type_t<false, T>
	{
		typedef T false_type;
	};

	// Implement compile-time selection of function based on
	// SFINAE and "compare_priorities" function existence
	template <typename T, typename V>
	struct priority_exists_t
	{
		typedef char yes;
		typedef int no;

		template <typename U, U> struct check;

		template <typename _1>
		static yes check_existence(check<int(T::*)(const V &, const V &) const,
		                                 &_1::compare_priorities> *);
		template <typename _1>
		static yes check_existence(check<int(*)(const V &, const V &),
		                                 &_1::compare_priorities> *);
		template <typename> no check_existence(...);

		enum { exists = (sizeof(check_existence<T>(nullptr)) == sizeof(yes)) };
	};

	// Implement compile-time selection of function based on
	// SFINAE and "processing_started" function existence
	template <typename T, typename V>
	struct start_handler_exists_t
	{
		typedef char yes;
		typedef int no;

		template <typename U, U> struct check;

		template <typename _1>
		static yes check_existence(check<void(T::*)(const V &) const, &_1::processing_started> *);
		template <typename _1>
		static yes check_existence(check<void(T::*)(const V &), &_1::processing_started> *);
		template <typename _1>
		static yes check_existence(check<void(*)(const V &), &_1::processing_started> *);
		template <typename>
		static no check_existence(...);

		enum { exists = (sizeof(check_existence<T>(nullptr)) == sizeof(yes)) };
	};

	// Implement compile-time selection of function based on
	// SFINAE and "processing_ended" function existence
	template <typename T, typename V>
	struct end_handler_exists_t
	{
		typedef char yes;
		typedef int no;

		template <typename U, U> struct check;

		template <typename _1>
		static yes check_existence(check<bool(T::*)(const V &) const, &_1::processing_ended> *);
		template <typename _1>
		static yes check_existence(check<bool(T::*)(const V &), &_1::processing_ended> *);
		template <typename _1>
		static yes check_existence(check<bool(*)(const V &), &_1::processing_ended> *);
		template <typename>
		static no check_existence(...);

		enum { exists = (sizeof(check_existence<T>(nullptr)) == sizeof(yes)) };
	};

	// Implement compile-time selection of function based on
	// SFINAE and "should_enqueue" function existence
	template <typename T, typename V, typename W>
	struct dedup_exists_t
	{
		typedef char yes;
		typedef int no;

		template <typename U, U> struct check;

		template <typename _1>
		static yes check_existence(check<bool(T::*)(const V &, const W &, const W &) const,
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(chec<bool(T::*)(const V &, const W &, const W &),
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(*)(const V &, const W &, const W &),
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(T::*)(const V &, W, const W &) const,
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(T::*)(const V &, W, const W &),
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(*)(const V &, W, const W &),
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(T::*)(const V &, const W &;, W) const,
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(T::*)(const V &, const W &, W),
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(*)(const V &, const W &, W),
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(T::*)(const V &, W, W) const,
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(T::*)(const V &, W, W),
		                                 &_1::should_enqueue> *);
		template <typename _1>
		static yes check_existence(check<bool(*)(const V &, W, W),
		                                 &_1::should_enqueue> *);

		template <typename>
		static no check_existence(...);

		enum { exists = (sizeof(check_existence<T>(0)) == sizeof(yes)) };
	};
}

template <typename T,
          typename Policy = detail::default_queue_policy<T>,
          typename Allocator = std::allocator<T> >
class mtqueue
{
private:
	struct node_t;

public:
	typedef T value_type;
	typedef Policy policy_type;
	typedef Allocator allocator_type;
	typedef mtqueue<value_type, policy_type, allocator_type> this_type;
	class iterator:
			public std::iterator<std::input_iterator_tag, value_type>
	{
	public:
		bool operator ==(const iterator & rhs) const
		{
			return current == rhs.current;
		}

		bool operator !=(const iterator & rhs) const
		{
			return current != rhs.current;
		}

		iterator & operator ++()
		{
			assert(current != 0);

			prev = current;
			current = current->next;

			return *this;
		}

		iterator operator ++(int)
		{
			assert(current != 0);

			iterator result(*this);

			++(*this);

			return result;
		}

		value_type & operator *() const
		{
			assert(current != 0);

			return current->value;
		}

		value_type * operator ->() const
		{
			assert(current != 0);

			return &current->value;
		}

	private:
		node_t * prev;
		node_t * current;

		iterator(node_t * node):
			prev(0),
			current(node)
		{}

		friend class mtqueue<value_type, policy_type, allocator_type>;
	};

	mtqueue():
		head(0),
		tail(0),
		shutdown(false)
	{}

	mtqueue(const policy_type & policy):
		head(0),
		tail(0),
		policy(policy),
		shutdown(false)
	{}

	mtqueue(const allocator_type & alloc):
		head(0),
		tail(0),
		alloc(alloc),
		shutdown(false)
	{}

	mtqueue(const policy_type & policy, const allocator_type & alloc):
		head(0),
		tail(0),
		policy(policy),
		alloc(alloc),
		shutdown(false)
	{}

	mtqueue(const this_type &) = delete;
	this_type & operator =(const this_type &) = delete;

    ~mtqueue()
	{
		while (head != 0) {
			node_t * next = head->next;

			alloc.destroy(head);
			alloc.deallocate(head, 1);
			head = next;
		}
	}

#if defined(__GXX_EXPERIMENTAL_CXX0X__)
	void enqueue(const value_type & val)
	{
		node_ptr node(alloc, 0);

		node.construct(val);

		enqueue<policy_type>(node);
	}

	void enqueue(value_type && val)
	{
		node_ptr node(alloc, 0);

		node.construct(std::move(val));

		enqueue<policy_type>(node);
	}
#endif

	// The following set of template functions should enqueue
	// newly constructed object in place.
	// This is the workaround for missing r-value references.
	template <typename Arg>
	void enqueue(const Arg & arg)
	{
		node_ptr node(alloc, 0);

		node.construct(arg);

		enqueue<policy_type>(node);
	}

	template <typename Arg1, typename Arg2>
	void enqueue(const Arg1 & arg1, const Arg2 & arg2)
	{
		node_ptr node(alloc, 0);

		node.construct(arg1, arg2);

		enqueue<policy_type>(node);
	}

	template <typename Arg1, typename Arg2, typename Arg3>
	void enqueue(const Arg1 & arg1, const Arg2 & arg2, const Arg3 & arg3)
	{
		node_ptr node(alloc, 0);

		node.construct(arg1, arg2, arg3);

		enqueue<policy_type>(node);
	}

	template <typename Arg1, typename Arg2, typename Arg3, typename Arg4>
	void enqueue(const Arg1 & arg1, const Arg2 & arg2, const Arg3 & arg3, const Arg4 & arg4)
	{
		node_ptr node(alloc, 0);

		node.construct(arg1, arg2, arg3, arg4);

		enqueue<policy_type>(node);
	}

	template <typename Processor>
	void process_queue(const Processor & process, bool wait = true)
	{
		process_queue(fifo_t(), process, wait);
	}

	// Process queue items and wait if queue is empty
	template <typename Fetcher, typename Processor>
	void process_queue(Fetcher fetch, Processor process, bool wait = true)
	{
		node_ptr element(alloc);
		bool process_queue = true;

		while (process_queue) {
			{
				std::unique_lock lock(queue_mutex);

				if (element.get() != 0) {
					on_processing_ended<policy_type>(element->value);
					element.reset();
				}

				if (shutdown) {
					return;
				}

				const iterator end(0);
				iterator place = fetch(policy, iterator(head), end);

				if (place == end) {
					if (wait) {
						queue_cond.wait(lock);
						continue;
					} else {
						return;
					}
				}

				assert(place.current != 0);

				on_processing_started<policy_type>(place.current->value);

				// "element" and "prev" are owned by storage ("head_") until
				// "head_" reassigned.
				// Following operations are non-throwing so we can't lost element here.
				if (place.prev == 0) {
					assert(place.current == head);

					head = place.current->next;
					if (tail == place.current) {
						tail = 0;
					}
				} else {
					place.prev->next = place.current->next;
					if (tail == place.current) {
						tail = place.prev;
					}
				}

				element.reset(place.current);
			}

			// Now "element" is guarateed to destroy event in case iof exception.
			// But no throwing operation performed except calling "process" functor.
			// As result - we can't lost element. All responsibility of keeping element
			// in case of error is lies on implementor of "process" functor.

			if (!(process_queue = process(element->value))) {
				std::unique_lock lock(queue_mutex);

				on_processing_ended<policy_type>(element->value);
			}
		}
	}

	void shutdown_processing()
	{
		std::unique_lock lock(queue_mutex);

		if (!shutdown)
		{
			shutdown = true;
			queue_cond.notify_all();
		}
	}

	void reset_state()
	{
		std::unique_lock lock(queue_mutex);

		shutdown = false;
	}

private:
	class priority_less:
			public std::unary_function<value_type, bool>
	{
	public:
		priority_less(const policy_type & policy, const value_type & value):
			policy(policy),
			value(value)
		{}

		bool operator ()(const value_type & item) const
		{
			return policy.compare_priorities(value, item) > 0;
		}

	private:
		const policy_type & policy;
		const value_type & value;
	};

	struct fifo_t
	{
		iterator operator ()(const policy_type & /*policy*/,
		                     const iterator & begin,
		                     const iterator & /*end*/) const
		{
			return begin;
		}
	};

	struct node_t
	{
		value_type value;
		node_t * next;

#if defined(__GXX_EXPERIMENTAL_CXX0X__)
		node_t(const value_type & value):
			value(value),
			next(0)
		{}

		node_t(value_type && value):
			value(std::move(value)),
			next(0)
		{}
#endif

		// The following set of template functions should
		// enqueue newly constructed object in place.
		// This is the workaround for missing r-value references.
		template <typename Arg>
		node_t(const Arg & arg):
			value(arg),
			next(0)
		{}

		template <typename Arg1, typename Arg2>
		node_t(const Arg1 & arg1, const Arg2 & arg2):
			value(arg1, arg2),
			next(0)
		{}

		template <typename Arg1, typename Arg2, typename Arg3>
		node_t(const Arg1 & arg1, const Arg2 & arg2, const Arg3 & arg3):
			value(arg1, arg2, arg3),
			next(0)
		{}

		template <typename Arg1, typename Arg2, typename Arg3, typename Arg4>
		node_t(const Arg1 & arg1, const Arg2 & arg2, const Arg3 & arg3, const Arg4 & arg4):
			value(arg1, arg2, arg3, arg4),
			next(0)
		{}

	private:
		node_t(const node_t &);
		node_t & operator =(const node_t &);
	};

	typedef typename Allocator::template rebind<node_t>::other node_allocator;

	class node_ptr
	{
	public:
		node_ptr(node_allocator & allocator):
			ptr(0),
			alloc(allocator),
			constructed(false)
		{}

		node_ptr(node_allocator & allocator, void * hint):
			ptr(allocator.allocate(1, hint)),
			alloc(allocator),
			constructed(false)
		{}

		node_ptr(node_t * node, node_allocator & alloc):
			ptr(node),
			alloc(alloc),
			constructed(true)
		{}

		~node_ptr()
		{
			reset();
		}

		void reset(node_t * node = 0)
		{
			if (ptr != 0) {
				if (constructed) {
					alloc.destroy(ptr);
				}
				alloc.deallocate(ptr, 1);
			}

			ptr = node;
			constructed = (node != 0);
		}

#if defined(__GXX_EXPERIMENTAL_CXX0X__)
		void construct(const value_type & val)
		{
			assert((ptr != 0) && !constructed);

			alloc.construct(ptr, val);
			constructed = true;
		}

		void construct(value_type && val)
		{
			assert((ptr != 0) && !constructed);

			alloc.construct(ptr, std::move(val));
			constructed = true;
		}
#endif

		// The following set of template functions should
		// enqueue newly constructed object in place.
		// This is the workaround for missing r-value references.
		template <typename Arg>
		void construct(const Arg & arg)
		{
			assert((ptr != 0) && !constructed);

			new (ptr) node_t(arg);
			constructed = true;
		}

		template <typename Arg1, typename Arg2>
		void construct(const Arg1 & arg1, const Arg2 & arg2)
		{
			assert((ptr != 0) && !constructed);

			new (ptr) node_t(arg1, arg2);
			constructed = true;
		}

		template <typename Arg1, typename Arg2, typename Arg3>
		void construct(const Arg1 & arg1, const Arg2 & arg2, const Arg3 & arg3)
		{
			assert((ptr != 0) && !constructed);

			new (ptr) node_t(arg1, arg2, arg3);
			constructed = true;
		}

		template <typename Arg1, typename Arg2, typename Arg3, typename Arg4>
		void construct(const Arg1 & arg1, const Arg2 & arg2, const Arg3 & arg3, const Arg4 & arg4)
		{
			assert((ptr != 0) && !constructed);

			new (ptr) node_t(arg1, arg2, arg3, arg4);
			constructed = true;
		}

		node_t * detach()
		{
			node_t * result = ptr;

			ptr = 0;

			return result;
		}

		node_t * get() const
		{
			return ptr;
		}

		node_t & operator *() const
		{
			assert((ptr != 0) && constructed);

			return *ptr;
		}

		node_t * operator ->() const
		{
			assert((ptr != 0) && constructed);

			return ptr;
		}

	private:
		node_t * ptr;
		node_allocator & alloc;
		bool constructed;
	};

	node_t * head;
	node_t * tail;
	policy_type policy;
	node_allocator alloc;
	std::mutex queue_mutex;
	std::condition_variable queue_cond;
	volatile bool shutdown;

	// This function should be selected by SFINAE when priorities are not used
	template <typename Selector>
	typename detail::select_type_t<detail::priority_exists_t<Selector,
	                                                         value_type>::exists>::false_type
		enqueue(node_ptr & node)
	{
		std::unique_lock lock(queue_mutex);

		if (!shutdown && should_enqueue<policy_type>(node->value))
		{
			if (tail != 0) {
				tail->next = node.detach();
				tail = tail->next;
			} else {
				tail = head = node.detach();
			}

			// We can't optimize out this call when queue wasn't empty
			// because fetcher can prevent handling items.
			// So we wake up threads hoping that newly added item can be
			// processed.
			queue_cond.notify_all();
		}
	}

	// This function should be selected by SFINAE when priorities are used
	template <typename Selector>
	typename detail::select_type_t<detail::priority_exists_t<Selector,
	                                                         value_type>::exists>::true_type
		enqueue(node_ptr & node)
	{
		std::unique_lock lock(queue_mutex);

		if (!shutdown && should_enqueue<policy_type>(node->value))
		{
			iterator place = std::find_if(iterator(head), iterator(0),
			                              priority_less(policy, node->value));

			if (place.prev == 0) {
				// Insert to head
				node->next = head;
				head = node.get();
				if (tail == 0) {
					tail = node.get();
				}
			} else {
				node->next = place.prev->next;
				place.prev->next = node.get();
				if (node->next == 0) {
					tail = node.get();
				}
			}
			node.detach();

			// We can't optimize out this call when queue wasn't empty
			// because fetcher can prevent handling items.
			// So we wake up threads hoping that newly added item can be
			// processed.
			queue_cond.notify_all();
		}
	}

	// This function should be selected by SFINAE when no dedup handler exists
	template <typename Selector>
	typename detail::select_type_t<detail::dedup_exists_t<Selector, value_type, iterator>::exists,
	                                                      bool>::false_type
		should_enqueue(const value_type &)
	{
		return true;
	}

	// This function should be selected by SFINAE when dedup handler exists
	template <typename Selector>
	typename detail::select_type_t<detail::dedup_exists_t<Selector, value_type, iterator>::exists,
	                                                      bool>::true_type
		should_enqueue(const value_type & value)
	{
		return policy.should_enqueue(value, iterator(head), iterator(0));
	}

	// This function should be selected by SFINAE when no element start event handler exists
	template <typename Selector>
	typename detail::select_type_t<detail::start_handler_exists_t<Selector,
	                                                              value_type>::exists>::false_type
		on_processing_started(const value_type &)
	{}

	// This function should be selected by SFINAE when element start event handler exists
	template <typename Selector>
	typename detail::select_type_t<detail::start_handler_exists_t<Selector,
	                                                              value_type>::exists>::true_type
		on_processing_started(const value_type & value)
	{
		policy.processing_started(value);
	}

	// This function should be selected by SFINAE when no element end event handler exists
	template <typename Selector>
	typename detail::select_type_t<detail::end_handler_exists_t<Selector,
	                                                            value_type>::exists>::false_type
		on_processing_ended(const value_type &)
	{}

	// This function should be selected by SFINAE when element end event handler exists
	template <typename Selector>
	typename detail::select_type_t<detail::end_handler_exists_t<Selector,
	                                                            value_type>::exists>::true_type
		on_processing_ended(const value_type & value)
	{
		if (policy.processing_ended(value)) {
			// More than one thread can handle objects from queue.
			// So try to awake them.
			queue_cond.notify_all();
		}
	}
};
```

